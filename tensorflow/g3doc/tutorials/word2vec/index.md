# Vector Representations of Words <a class="md-anchor" id="AUTOGENERATED-vector-representations-of-words"></a>
# 词向量化表示<a class="md-anchor" id="AUTOGENERATED-vector-representations-of-words"></a> 

In this tutorial we look at the word2vec model by
[Mikolov et al.](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).
This model is used for learning vector representations of words, called *word
embeddings*.

本教程我们关注[Mikolov et al.](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). 的wod2vec模型
这个称为 *词嵌入* 的模型是用来学习词的向量化表示的。

## Highlights <a class="md-anchor" id="AUTOGENERATED-highlights"></a>
## 重头戏 <a class="md-anchor" id="AUTOGENERATED-highlights"></a>

This tutorial is meant to highlight the interesting, substantive parts of
building a word2vec model in TensorFlow.
本教程是为了突出在TensorFlow中构建word2vec模型中的有趣，真实的部分。


* We start by giving the motivation for why we would want to
represent words as vectors.
* We look at the intuition behind the model and how it is trained
(with a splash of math for good measure).
* We also show a simple implementation of the model in TensorFlow.
* Finally, we look at ways to make the naive version scale better.


* 我们从将词表示为向量的动机开始
* 我们关注模型背后的直觉，以及如何训练（同时会秀一点儿数学）
* 我们还会给出这个模型在TensorFlow里的一个简单实现
* 最后，我们看一下如何让这个简单的版本更好的扩展

We walk through the code later during the tutorial, but if you'd prefer to dive
straight in, feel free to look at the minimalistic implementation in
[tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py)
This basic example contains the code needed to download some data, train on it a
bit and visualize the result. Once you get comfortable with reading and running
the basic version, you can graduate to
[tensorflow/models/embedding/word2vec.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py)
which is a more serious implementation that showcases some more advanced
TensorFlow principles about how to efficiently use threads to move data into a
text model, how to checkpoint during training, etc.

我们会在稍后的教程中分析代码，如果你喜欢单刀直入的方式，也欢迎直接查看最简单的实现[tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py)
这个最简单的例子包含下载一些数据的代码，训练并将结果可视化。一旦你熟悉了阅读和运行基础版本后，你就可以晋级到更严谨的版本[tensorflow/models/embedding/word2vec.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py)
这里会多一些TensorFlow的高级原则，比如如何高效使用线程来将数据移至文本模型，如何在训练时设置断点等

But first, let's look at why we would want to learn word embeddings in the first
place. Feel free to skip this section if you're an Embedding Pro and you'd just
like to get your hands dirty with the details.

但是，首先，我们需要先看一下我们为什么需要词嵌入。如果你已经是这方面的专家，想马上就动手，请跳过本章

## Motivation: Why Learn Word Embeddings? <a class="md-anchor" id="AUTOGENERATED-motivation--why-learn-word-embeddings-"></a>
## 动机：为什么要词嵌入？<a class="md-anchor" id="AUTOGENERATED-motivation--why-learn-word-embeddings-"></a>

Image and audio processing systems work with rich, high-dimensional datasets
encoded as vectors of the individual raw pixel-intensities for image data, or
e.g. power spectral density coefficients for audio data. For tasks like object
or speech recognition we know that all the information required to successfully
perform the task is encoded in the data (because humans can perform these tasks
from the raw data).  However, natural language processing systems traditionally
treat words as discrete atomic symbols, and therefore 'cat' may be represented
as  `Id537` and 'dog' as `Id143`.  These encodings are arbitrary, and provide
no useful information to the system regarding the relationships that may exist
between the individual symbols. This means that the model can leverage
very little of what it has learned about 'cats' when it is processing data about
'dogs' (such that they are both animals, four-legged, pets, etc.). Representing
words as unique, discrete ids furthermore leads to data sparsity, and usually
means that we may need more data in order to successfully train statistical
models.  Using vector representations can overcome some of these obstacles.

在图片和音频处理系统中，高维的数据集都被编码为向量,比如图片的像素，音频的功率密度系数。
对于对象或语音识别，我们知道所有的信息都被编码为数据(because humans can perform these tasks from the raw data)
但是自然语言处理系统传统上都将词当成一个离散的原子符号，因此'cat'被表示为`Id537`,`dog`被表示为`Id143`。
这种编码是随意的，无法表达存在与两个符号之间的关系信息。这也意味着系统在处理`dogs`时，无法利用它已经在`cat`上学习的信息（比如他们都是
动物，都有4条腿，都是宠物等）。将词表示为唯一的，离散的Ids还会导致数据稀疏，这也通常意味着我们需要更多的数据来训练统计模型。使用向量表
示可以克服这些障碍。



<div style="width:100%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="img/audio-image-text.png" alt>
</div>


[Vector space models](https://en.wikipedia.org/wiki/Vector_space_model) (VSMs)
represent (embed) words in a continuous vector space where semantically
similar words are mapped to nearby points ('are embedded nearby each other').
VSMs have a long, rich history in NLP, but all methods depend in some way or
another on the
[Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis),
which states that words that appear in the same contexts share
semantic meaning. The different approaches that leverage this principle can be
divided into two categories: *count-based methods* (e.g.
[Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)),
and *predictive methods* (e.g.
[neural probabilistic language models](http://www.scholarpedia.org/article/Neural_net_language_models)).

[向量空间模型](https://en.wikipedia.org/wiki/Vector_space_model) (VSMs)
将词在一个连续空间里表示，相似的词映射为距离较近的点('彼此嵌入') VSM在NLP领域里有着悠久的历史，但是，所有的方法都在某些角度上都依赖[Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis),
它声明在相同上下文出现过的词共享语义。有两种利用这个原则的方式： *基与计数的方法(count-based methods)* (比如[潜在语义分析(Latent Semantic Analysis)](https://en.wikipedia.org/wiki/Latent_semantic_analysis)),
和 *预估方法(predictive methods)* (比如[神经语言概率模型(neural probabilistic language models)](http://www.scholarpedia.org/article/Neural_net_language_models)).

This distinction is elaborated in much more detail by
[Baroni et al.](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf),
but in a nutshell: Count-based methods compute the statistics of
how often some word co-occurs with its neighbor words in a large text corpus,
and then map these count-statistics down to a small, dense vector for each word.
Predictive models directly try to predict a word from its neighbors in terms of
learned small, dense *embedding vectors* (considered parameters of the
model).

[Baroni et al.](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf)对它们的区别
有一个非常详尽的描述，总的来说:基于计数的方法会在一个大的文本语料库里统计每个词和其他词作为邻居共现的次数，然后将这些统计计数沉淀为每个词
的小的稠密(dense)向量。预估方法直接试图根据它的邻居直接预估一个词的小的稠密(dense) *嵌入向量* (考虑模型的参数)。

Word2vec is a particularly computationally-efficient predictive model for
learning word embeddings from raw text. It comes in two flavors, the Continuous
Bag-of-Words model (CBOW) and the Skip-Gram model. Algorithmically, these
models are similar, except that CBOW predicts target words (e.g. 'mat') from
source context words ('the cat sits on the'), while the skip-gram does the
inverse and predicts source context-words from the target words. This inversion
might seem like an arbitrary choice, but statistically it has the effect that
CBOW smoothes over a lot of the distributional information (by treating an
entire context as one observation). For the most part, this turns out to be a
useful thing for smaller datasets. However, skip-gram treats each context-target
pair as a new observation, and this tends to do better when we have larger
datasets. We will focus on the skip-gram model in the rest of this tutorial.

Word2vec是一个从原始文本学习词嵌入的非常高效的预估模型。它有两个模型： 持续词袋模型(CBOW)和Skip-Gram模型。在算法上，两个模型是相似的，
不同的是CBOW模型从源上下文的词('the cat sits on the')预测目标词(比如'mat'),而skip-gram作相反的预测，即从目标词预测源上下文。
这个反转看起来像是随意的，但从统计学上来说，它使得CBOW的结果更加平滑（通过将整个上下文作为以此观察）。大多数情况下，这对于小数据集非常
有用。但是skip-gram将每个上下文-目标词的组合作为一次新的观察，这在我们拥有一个大数据集时效果更好。本文的剩余部分主要关注skip-gram模型


## Scaling up with Noise-Contrastive Training <a class="md-anchor" id="AUTOGENERATED-scaling-up-with-noise-contrastive-training"></a>
## 从噪声-对比训练开始扩展 <a class="md-anchor" id="AUTOGENERATED-scaling-up-with-noise-contrastive-training"></a>

Neural probabilistic language models are traditionally trained using the
[maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood) (ML)
principle  to maximize the probability of the next word \\(w_t\\) (for 'target)
given the previous words \\(h\\) (for 'history') in terms of a
[*softmax* function](https://en.wikipedia.org/wiki/Softmax_function),

神经语言概率模型传统上一般使用[最大似然(maximum likelihood)](https://en.wikipedia.org/wiki/Maximum_likelihood) (ML)原则，
采用[*softmax* function](https://en.wikipedia.org/wiki/Softmax_function) 的形式来最大化给出历史词 \\(h\\) (即历史) 的下个词
\\(w_t\\) (即目标词)的概率。

$$
\begin{align}
P(w_t | h) &= \text{softmax}(\exp \{ \text{score}(w_t, h) \}) \\
           &= \frac{\exp \{ \text{score}(w_t, h) \} }
             {\sum_\text{Word w' in Vocab} \exp \{ \text{score}(w', h) \} }.
\end{align}
$$

![](http://latex.codecogs.com/gif.latex?%5Cbegin%7Balign%7D%20P%28w_t%20%7C%20h%29%20%26%3D%20%5Ctext%7Bsoftmax%7D%28%5Cexp%20%5C%7B%20%5Ctext%7Bscore%7D%28w_t%2C%20h%29%20%5C%7D%29%20%5C%5C%20%26%3D%20%5Cfrac%7B%5Cexp%20%5C%7B%20%5Ctext%7Bscore%7D%28w_t%2C%20h%29%20%5C%7D%20%7D%20%7B%5Csum_%5Ctext%7BWord%20w%27%20in%20Vocab%7D%20%5Cexp%20%5C%7B%20%5Ctext%7Bscore%7D%28w%27%2C%20h%29%20%5C%7D%20%7D.%20%5Cend%7Balign%7D)



where \\(\text{score}(w_t, h)\\) computes the compatibility of word \\(w_t\\) with
the context \\(h\\) (a dot product is commonly used). We train this model by
maximizing its log-likelihood on the training set, i.e. by maximizing

这里的 \\(\text{score}(w_t, h)\\) 计算了词 \\(w_t\\)  和 上下文 \\(h\\) 的兼容性（一般使用点积计算),我们通过最大化训练数据上
的log-likelihood来训练模型，比如通过最大化

$$
\begin{align}
 J_\text{ML} &= \log P(w_t | h) \\
  &= \text{score}(w_t, h) -
     \log \left( \sum_\text{Word w' in Vocab} \exp \{ \text{score}(w', h) \} \right)
\end{align}
$$

!![](http://latex.codecogs.com/gif.latex?%5Cbegin%7Balign%7D%20J_%5Ctext%7BML%7D%20%26%3D%20%5Clog%20P%28w_t%20%7C%20h%29%20%5C%5C%20%26%3D%20%5Ctext%7Bscore%7D%28w_t%2C%20h%29%20-%20%5Clog%20%5Cleft%28%20%5Csum_%5Ctext%7BWord%20w%27%20in%20Vocab%7D%20%5Cexp%20%5C%7B%20%5Ctext%7Bscore%7D%28w%27%2C%20h%29%20%5C%7D%20%5Cright%29%20%5Cend%7Balign%7D)

This yields a properly normalized probabilistic model for language modeling.
However this is very expensive, because we need to compute and normalize each
probability using the score for all other \\(V\\) words \\(w'\\) in the current
context \\(h\\), *at every training step*.

这就为语言模型构造了一个恰当的归一化的概率模型。但是，这个模型非常昂贵，因为我们需要在 *每一步计算中* 使用所有上下文中非当前词的得分，
计算和归一化词的概率

<div style="width:60%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="img/softmax-nplm.png" alt>
</div>

On the other hand, for feature learning in word2vec we do not need a full
probabilistic model. The CBOW and skip-gram models are instead trained using a
binary classification objective (logistic regression) to discriminate the real
target words \\(w_t\\) from \\(k\\) imaginary (noise) words \\(\tilde w\\), in the
same context. We illustrate this below for a CBOW model. For skip-gram the
direction is simply inverted.

另一方面，word2vec里的特征训练不需要一个完整的概率模型。CBOW和skip-gram模型使用一个二分类目标（逻辑回归）来区分同一个上下文中的
实词 ![](http://latex.codecogs.com/gif.latex?%5C%5C%28w_t%5C%29) 
和(k) 虚词 (噪音) ![](http://latex.codecogs.com/gif.latex?%5C%5C%28%5Ctilde%20w%29)。
我们在下面有对CBOW模型的解释，对于skip-gram，只是方向相反而已。

<div style="width:60%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="img/nce-nplm.png" alt>
</div>

Mathematically, the objective (for each example) is to maximize


$$J_\text{NEG} = \log Q_\theta(D=1 |w_t, h) +
  k \mathop{\mathbb{E}}_{\tilde w \sim P_\text{noise}}
     \left[ \log Q_\theta(D = 0 |\tilde w, h) \right]$$,

从数学的角度讲，目标（对每个例子来说)是最大化:![](http://latex.codecogs.com/gif.latex?%24%24J_%5Ctext%7BNEG%7D%20%3D%20%5Clog%20Q_%5Ctheta%28D%3D1%20%7Cw_t%2C%20h%29%20&plus;%20k%20%5Cmathop%7B%5Cmathbb%7BE%7D%7D_%7B%5Ctilde%20w%20%5Csim%20P_%5Ctext%7Bnoise%7D%7D%20%5Cleft%5B%20%5Clog%20Q_%5Ctheta%28D%20%3D%200%20%7C%5Ctilde%20w%2C%20h%29%20%5Cright%5D%24%24%2C)


where \\(Q_\theta(D=1 | w, h)\\) is the binary logistic regression probability
under the model of seeing the word \\(w\\) in the context \\(h\\) in the dataset
\\(D\\), calculated in terms of the learned embedding vectors \\(\theta\\). In
practice we approximate the expectation by drawing \\(k\\) constrastive words
from the noise distribution (i.e. we compute a
[Monte Carlo average](https://en.wikipedia.org/wiki/Monte_Carlo_integration)).


这里的 ![](http://latex.codecogs.com/gif.latex?%5C%5C%28Q_%5Ctheta%28D%3D1%20%7C%20w%2C%20h%29%29)是在数据集中，根据学习到
的嵌入词向量计算的,看到词(w)在上下文(h)中出现的二分逻辑回归的概率。在实践中，we approximate the expectation by drawing
 \\(k\\) constrastive words from the noise distribution (i.e. we compute a 
 [Monte Carlo average](https://en.wikipedia.org/wiki/Monte_Carlo_integration)).

This objective is maximized when the model assigns high probabilities
to the real words, and low probabilities to noise words. Technically, this is
called
[Negative Sampling](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf),
and there is good mathematical motivation for using this loss function:
The updates it proposes approximate the updates of the softmax function in the
limit. But computationally it is especially appealing because computing the
loss function now scales only with the number of *noise words* that we
select (\\(k\\)), and not *all words* in the vocabulary (\\(V\\)). This makes it
much faster to train. We will actually make use of the very similar
[noise-contrastive estimation (NCE)](http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf)
loss, for which TensorFlow has a handy helper function `tf.nn.nce_loss()`.
当模型给实词高的概率，而给噪音词低的概率时，目标是最大化的。这在技术上称为[负采样(Negative Sampling)](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf),
有一个使用这个损失函数的非常好的数学动机：它提议的更新近似与softmax函数的更新的极限。但是，在计算上它非常吸引人，因为计算损失函数的复杂
度现在只和我们选择的 *噪音词* 的个数(\\(k\\))相关，而不是跟词典(\\(V\\))里的 *所有词* 相关。这样子就可以更快的训练。我们实际上会使用
[对比噪音估算(noise-contrastive estimation (NCE))](http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf)
，这个在TensorFLow里有个方便的帮助函数`tf.nn.nce_loss()`.

Let's get an intuitive feel for how this would work in practice!

让我们直观的感受一下上面说的在实践中是如何工作的!

## The Skip-gram Model <a class="md-anchor" id="AUTOGENERATED-the-skip-gram-model"></a>

As an example, let's consider the dataset

我们考虑下面的数据集作为例子

`the quick brown fox jumped over the lazy dog`


We first form a dataset of words and the contexts in which they appear. We
could define 'context' in any way that makes sense, and in fact people have
looked at syntactic contexts (i.e. the syntactic dependents of the current
target word, see e.g.
[Levy et al.](https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf)),
words-to-the-left of the target, words-to-the-right of the target, etc. For now,
let's stick to the vanilla definition and define 'context' as the window
of words to the left and to the right of a target word. Using a window
size of 1, we then have the dataset

我们先作一个词和上下文的数据集。我们可以任何有意义的'上下文',事实上，人们已经对比了语法上下文(比如，当前词的语法依存，参考[Levy et al.](https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf)),
目标词的左侧词，目标词的右侧词等方式。现在，我们使用vanilla定义的窗口上下文，即目标词从左到右的窗口。如果窗口大小为1，则我们的数据集如下

`([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...`

of `(context, target)` pairs. Recall that skip-gram inverts contexts and
targets, and tries to predict each context word from its target word, so the
task becomes to predict 'the' and 'brown' from 'quick', 'quick' and 'fox' from
'brown', etc. Therefore our dataset becomes

记住skip-gram反转了上下文和目标词，试图使用每个词来预测上下文，所以，任务就变为了根据'quick'来预测'the'和'brown',根据'brown'
预测'quick'和'fox',因此，我们的数据集就变成了:

`(quick, the), (quick, brown), (brown, quick), (brown, fox), ...`

of `(input, output)` pairs.  The objective function is defined over the entire
dataset, but we typically optimize this with
[stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
(SGD) using one example at a time (or a 'minibatch' of `batch_size` examples,
where typically `16 <= batch_size <= 512`). So let's look at one step of
this process.

目标函数是在整个数据集上定义的，但是，我们使用经典的[随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
(SGD),每次使用一个例子（或者每次使用一小批例子,批次大小为 `batch_size` ，它一般在16到512之间`16 <= batch_size <= 512`)。
现在，我们看一下这个处理的一步。

Let's imagine at training step \\(t\\) we observe the first training case above,
where the goal is to predict `the` from `quick`. We select `num_noise` number
of noisy (contrastive) examples by drawing from some noise distribution,
typically the unigram distribution, \\(P(w)\\). For simplicity let's say
`num_noise=1` and we select `sheep` as a noisy example. Next we compute the
loss for this pair of observed and noisy examples, i.e. the objective at time
step \\(t\\) becomes

首先想象我们在第\\(t\\)步，我们观察到了上面的训练case，目标是根据`quick`预测`the`。我们选择`num_noise`个噪音(对比)例子，通过绘制某种
噪音分布，典型的如一元分布(unigram distribution），概率为\\(P(w)\\)。简单起见，我们选择`num_noise=1`,然后选择`sheep`作为噪音例子。
下一步，我们计算这对词和噪音例子的损失。第 \\(t\\)的目标函数变为:

$$J^{(t)}_\text{NEG} = \log Q_\theta(D=1 | \text{the, quick}) +
  \log(Q_\theta(D=0 | \text{sheep, quick}))$$.
 
![](http://latex.codecogs.com/gif.latex?%24%24J%5E%7B%28t%29%7D_%5Ctext%7BNEG%7D%20%3D%20%5Clog%20Q_%5Ctheta%28D%3D1%20%7C%20%5Ctext%7Bthe%2C%20quick%7D%29%20&plus;%20%5Clog%28Q_%5Ctheta%28D%3D0%20%7C%20%5Ctext%7Bsheep%2C%20quick%7D%29%29%24%24.)

The goal is to make an update to the embedding parameters \\(\theta\\) to improve
(in this case, maximize) this objective function.  We do this by deriving the
gradient of the loss with respect to the embedding parameters \\(\theta\\), i.e.
\\(\frac{\partial}{\partial \theta} J_\text{NEG}\\) (luckily TensorFlow provides
easy helper functions for doing this!). We then perform an update to the
embeddings by taking a small step in the direction of the gradient. When this
process is repeated over the entire training set, this has the effect of
'moving' the embedding vectors around for each word until the model is
successful at discriminating real words from noise words.

目标是修改嵌入参数 \\(\theta\\) 来改进目标函数（本例中即最大化目标函数)。我们通过目标参数\\(\theta\\)得到损失函数的梯度，也就是，
![](http://latex.codecogs.com/gif.latex?%5C%5C%28%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Ctheta%7D%20J_%5Ctext%7BNEG%7D%29)
(幸运的是TensorFlow提供了这么作的辅助函数!)。然我们通过在梯度方向上前进一小步来执行嵌入的更新。当这样的操作在整个数据集上重复的时候，就
会慢慢的移动每个词向量直到成功的区分出实词和噪音词。

We can visualize the learned vectors by projecting them down to 2 dimensions
using for instance something like the
[t-SNE dimensionality reduction technique](http://lvdmaaten.github.io/tsne/).
When we inspect these visualizations it becomes apparent that the vectors
capture some general, and in fact quite useful, semantic information about
words and their relationships to one another. It was very interesting when we
first discovered that certain directions in the induced vector space specialize
towards certain semantic relationships, e.g. *male-female*, *gender* and
even *country-capital* relationships between words, as illustrated in the figure
below (see also for example
[Mikolov et al., 2013](http://www.aclweb.org/anthology/N13-1090)).

我们可以通过使用类似[t-SNE dimensionality reduction technique](http://lvdmaaten.github.io/tsne/)的方法将他们投影到2维空间来可
视化。当我们观察这个可视化时，就好像向量获得了它和其他词关系的某种通用的，而且非常有用的信息。当我们第一次发现某些方向定义了某些语法关系
比如 *male-female*,*gener* 甚至 *country-capital*的关系，如下图所示(see also for example [Mikolov et al., 2013](http://www.aclweb.org/anthology/N13-1090)).

<div style="width:100%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="img/linear-relationships.png" alt>
</div>

This explains why these vectors are also useful as features for many canonical
NLP prediction tasks, such as part-of-speech tagging or named entity recognition
(see for example the original work by
[Collobert et al.](http://arxiv.org/pdf/1103.0398v1.pdf), or follow-up work by
[Turian et al.](http://www.aclweb.org/anthology/P10-1040)).

这就解释了为什么这些向量在很多NLP预测任务里都是非常有用的特征。比如词性标注(part-of-speech tagging)或者命名实体时别(named entity recognition)
(see for example the original work by
[Collobert et al.](http://arxiv.org/pdf/1103.0398v1.pdf), or follow-up work by
[Turian et al.](http://www.aclweb.org/anthology/P10-1040)).

But for now, let's just use them to draw pretty pictures!

现在，我们开始画一下这个有趣的图!

## Building the Graph <a class="md-anchor" id="AUTOGENERATED-building-the-graph"></a>
## 画图 <a class="md-anchor" id="AUTOGENERATED-building-the-graph"></a>

This is all about embeddings, so let's define our embedding matrix.
This is just a big random matrix to start.  We'll initialize the values to be
uniform in the unit cube.

我们一直在讲嵌入，所以我们来定义我们的嵌入矩阵。首先从一个随机的矩阵开始，我们用1到-1之间的数填充

```python
embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
```

The noise-contrastive estimation loss is defined in terms a logistic regression
model. For this, we need to define the weights and biases for each word in the
vocabulary (also called the `output weights` as opposed to the `input
embeddings`). So let's define that.

对比噪音预估损失使用逻辑回归模型构建。为此，我们需要定义词典里每个词的权重和偏差（`output weights`也被称为`input embeddings`).
，如下是定义:

```python
nce_weights = tf.Variable(
  tf.truncated_normal([vocabulary_size, embedding_size],
                      stddev=1.0 / math.sqrt(embedding_size)))
nce_biases = tf.Variable(tf.zeros([vocabulary_size]))
```

Now that we have the parameters in place, we can define our skip-gram model
graph. For simplicity, let's suppose we've already integerized our text corpus
with a vocabulary so that each word is represented as an integer (see
[tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py)
for the details). The skip-gram model takes two inputs. One is a batch full of
integers representing the source context words, the other is for the target
words. Let's create placeholder nodes for these inputs, so that we can feed in
data later.

现在，我们的参数都到位了，我们可以定义我们的skip-gram模型图了，简单起见，我们假设我们已经使用词典整数化了我们的文本，所以每个词都会表示
为一个整数（参考[tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py) for the details).
skip-gram有两个输入，一个是批量的整数化表示源上下文的词，另一个是目标词。我们为这些输入创建一个占位符，这样我们可以在后面填充它们。

```python
# Placeholders for inputs
train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
```

Now what we need to do is look up the vector for each of the source words in
the batch.  TensorFlow has handy helpers that make this easy.

现在，我们需要批量查看每个源词的向量，TensorFlow有一个辅助函数来简化这个操作

```python
embed = tf.nn.embedding_lookup(embeddings, train_inputs)
```

Ok, now that we have the embeddings for each word, we'd like to try to predict
the target word using the noise-contrastive training objective.

现在，我们对每个词都有嵌入了，我们希望使用噪音对比训练目标来预测目标词。

```python
# Compute the NCE loss, using a sample of the negative labels each time.
loss = tf.reduce_mean(
  tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,
                 num_sampled, vocabulary_size))
```

Now that we have a loss node, we need to add the nodes required to compute
gradients and update the parameters, etc. For this we will use stochastic
gradient descent, and TensorFlow has handy helpers to make this easy.

现在，我们有一个损失节点，我们需要给这个节点添加必须的梯度计算和参数更新操作等。为此，我们使用随机梯度下降，TensorFlow的辅助函数同样简
化了这个操作

```python
# We use the SGD optimizer.
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)
```

## Training the Model <a class="md-anchor" id="AUTOGENERATED-training-the-model"></a>
## 训练模型

Training the model is then as simple as using a `feed_dict` to push data into
the placeholders and calling `session.run` with this new data in a loop.

训练模型就是简单的使用`feed_dict` 来将数据推入占位符，然后调用对这个数据循环调用`session.run`。 

```python
for inputs, labels in generate_batch(...):
  feed_dict = {training_inputs: inputs, training_labels: labels}
  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)
```

See the full example code in
[tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py](./word2vec_basic.py).

完整的示例代码参考[tensorflow/g3doc/tutorials/word2vec/word2vec_basic.py](./word2vec_basic.py).

## Visualizing the Learned Embeddings <a class="md-anchor" id="AUTOGENERATED-visualizing-the-learned-embeddings"></a>
## 可视化嵌入学习<a class="md-anchor" id="AUTOGENERATED-visualizing-the-learned-embeddings"></a>

After training has finished we can visualize the learned embeddings using
t-SNE.

训练结束后，我们可以是哟哦嗯t-SNE来可视化嵌入的学习

<div style="width:100%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="img/tsne.png" alt>
</div>

美景尽收! 和预期一样，相似的词被聚到了一起。对word2vec更重量级的实现会展示出TensorFlow的更多高级特性，参考
Et voila! As expected, words that are similar end up clustering nearby each
other. For a more heavyweight implementation of word2vec that showcases more of[tensorflow/models/embedding/word2vec.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py).
the advanced features of TensorFlow, see the implementation in
[tensorflow/models/embedding/word2vec.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py).

## Evaluating Embeddings: Analogical Reasoning <a class="md-anchor" id="AUTOGENERATED-evaluating-embeddings--analogical-reasoning"></a>
## 评估嵌入：类比推理 <a class="md-anchor" id="AUTOGENERATED-evaluating-embeddings--analogical-reasoning"></a>

Embeddings are useful for a wide variety of prediction tasks in NLP. Short of
training a full-blown part-of-speech model or named-entity model, one simple way
to evaluate embeddings is to directly use them to predict syntactic and semantic
relationships like `king is to queen as father is to ?`. This is called
*analogical reasoning* and the task was introduced by
[Mikolov and colleagues](http://msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf),
and the dataset can be downloaded from here:
https://word2vec.googlecode.com/svn/trunk/questions-words.txt.
词嵌入在NLP的很多预测任务里都非常有用，除了在训练full-blown词性模型或则命名实体模型以外，一个简单的方式是直接使用他们来推测语义和语法的
关系，了例如 `king is to queen as father is to ?`. 这被称为 *类比推理* ,这个任务在Mikolov and colleagues](http://msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf),
里有介绍，数据集可以这里下载:https://word2vec.googlecode.com/svn/trunk/questions-words.txt.

To see how we do this evaluation, have a look at the `build_eval_graph()` and
`eval()` functions in
[tensorflow/models/embedding/word2vec.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py).

可以通过[tensorflow/models/embedding/word2vec.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py).
里的`build_eval_graph()` 和`eval()` 函数看一下如何这个推测。

The choice of hyperparameters can strongly influence the accuracy on this task.
To achieve state-of-the-art performance on this task requires training over a
very large dataset, carefully tuning the hyperparameters and making use of
tricks like subsampling the data, which is out of the scope of this tutorial.

超参的选择会很大程度上影响任务的精度。想要在这个任务上取得state-of-the-art performance，需要在一个很大的数据集上训练，然后小心的优化
超参，利用类似子集采样(subsampling)的tricks，这超出了本教程的范围。



## Optimizing the Implementation <a class="md-anchor" id="AUTOGENERATED-optimizing-the-implementation"></a>
## 优化实现<a class="md-anchor" id="AUTOGENERATED-optimizing-the-implementation"></a>

Our vanilla implementation showcases the flexibility of TensorFlow. For
example, changing the training objective is as simple as swapping out the call
to `tf.nn.nce_loss()` for an off-the-shelf alternative such as
`tf.nn.sampled_softmax_loss()`. If you have a new idea for a loss function, you
can manually write an expression for the new objective in TensorFlow and let
the optimizer compute its derivatives. This flexibility is invaluable in the
exploratory phase of machine learning model development, where we are trying
out several different ideas and iterating quickly.

我们的vanilla实现展示了TensorFlow的灵活性，比如，如果需要修改目标，只需要擦掉 `tf.nn.nce_loss()` ，替换为一个类似
`tf.nn.sampled_softmax_loss()`的函数。如果你有新的损失函数，你可以手动为这个新的目标写表达式，然后让优化器来来优化计算。
这个灵活性在机器学习研发的探索阶段是非常有用的，这让我们可以实验各种想法和快数迭代。

Once you have a model structure you're satisfied with, it may be worth
optimizing your implementation to run more efficiently (and cover more data in
less time).  For example, the naive code we used in this tutorial would suffer
compromised speed because we use Python for reading and feeding data items --
each of which require very little work on the TensorFlow back-end.  If you find
your model is seriously bottlenecked on input data, you may want to implement a
custom data reader for your problem, as described in
[New Data Formats](../../how_tos/new_data_formats/index.md).  For the case of Skip-Gram
modeling, we've actually already done this for you as an example in
[tensorflow/models/embedding/word2vec.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py).

一旦你发现了你中意的模型结构，就值的为其优化性能（使用更少的时间,高效的处理更多的数据）。比如，我们本教程中使用的是一种折中的方式，我们
使用Python来读取和填充数据 -- 他们都只需要TensorFlow在背后作一点点儿工作。如果你发现你的瓶颈在输入数据，你就需要实现一个自定义的reader
就像在[New Data Formats](../../how_tos/new_data_formats/index.md)里描述的那样。对于Skip-Gram模型，我们已经在例子中实现了。
[tensorflow/models/embedding/word2vec.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec.py).

If your model is no longer I/O bound but you want still more performance, you
can take things further by writing your own TensorFlow Ops, as described in
[Adding a New Op](../../how_tos/adding_an_op/index.md).  Again we've provided an
example of this for the Skip-Gram case
[tensorflow/models/embedding/word2vec_optimized.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec_optimized.py).
Feel free to benchmark these against each other to measure performance
improvements at each stage.[tensorflow/models/embedding/word2vec_optimized.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec_optimized.py).

如果你的模型已经不是IO瓶颈了，但你还想进一步提高性能。你可以通过写你自己的TensorFlow Ops来更进一步。就像在[Adding a New Op](../../how_tos/adding_an_op/index.md)
里描述的那样。当然，这对于Skip-Gram我们也提供了实现:[tensorflow/models/embedding/word2vec_optimized.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec_optimized.py).
你可以随意对比在每个阶段的性能。[tensorflow/models/embedding/word2vec_optimized.py](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/embedding/word2vec_optimized.py).

## Conclusion <a class="md-anchor" id="AUTOGENERATED-conclusion"></a>
## 结论 <a class="md-anchor" id="AUTOGENERATED-conclusion"></a>

In this tutorial we covered the word2vec model, a computationally efficient
model for learning word embeddings. We motivated why embeddings are useful,
discussed efficient training techniques and showed how to implement all of this
in TensorFlow. Overall, we hope that this has show-cased how TensorFlow affords
you the flexibility you need for early experimentation, and the control you
later need for bespoke optimized implementation.

在本教程中我们涵盖了word2vec的模型，一个高效的学习词向量的模型。我们从它为什么有用入手，讨论了高效训练的方法，并给出了在TensorFlow
里的实现。总的来说，我们希望能够给你展示TensorFlow可以给你在早期实现中给你带来的便利性，以及你在日后自定优化实现的控制。
